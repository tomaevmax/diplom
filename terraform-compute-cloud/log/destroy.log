[0m[1myandex_iam_service_account.bucket-account: Refreshing state... [id=aje899k49l4ri8fojku7][0m
[0m[1myandex_dns_zone.diplom: Refreshing state... [id=dns39tr9k9127j7d38ak][0m
[0m[1myandex_iam_service_account.tech-account: Refreshing state... [id=ajep7io5sk5fidb8vvii][0m
[0m[1myandex_vpc_network.my-k8s-net: Refreshing state... [id=enp5ob0gdnqnuq41a4ob][0m
[0m[1myandex_compute_image.ubuntu-test: Refreshing state... [id=fd8vjq5ab5g28bj5bojc][0m
[0m[1myandex_resourcemanager_folder_iam_member.bucket-account: Refreshing state... [id=b1gb1aal3vgk7p7nr6nd/storage.admin/serviceAccount:aje899k49l4ri8fojku7][0m
[0m[1myandex_iam_service_account_static_access_key.bucket-account-key: Refreshing state... [id=aje8r7pdjjhqi6up1qum][0m
[0m[1myandex_resourcemanager_folder_iam_member.k8s-editor: Refreshing state... [id=b1gb1aal3vgk7p7nr6nd/editor/serviceAccount:ajep7io5sk5fidb8vvii][0m
[0m[1myandex_vpc_subnet.mysubnet-b: Refreshing state... [id=e2l8kesa74sp6dcqgib1][0m
[0m[1myandex_vpc_subnet.mysubnet-d: Refreshing state... [id=fl84j1g960uevkr0vq2g][0m
[0m[1myandex_vpc_subnet.mysubnet-a: Refreshing state... [id=e9b4dh3mco90q2druajv][0m
[0m[1myandex_storage_bucket.project-bucket: Refreshing state... [id=k8s-tech][0m
[0m[1myandex_compute_instance_group.k8s-node: Refreshing state... [id=cl1eq71t61q4uqudcs0m][0m
[0m[1myandex_dns_recordset.registry: Refreshing state... [id=dns39tr9k9127j7d38ak/registry.tomaev-maksim.ru./A][0m
[0m[1myandex_dns_recordset.test-app: Refreshing state... [id=dns39tr9k9127j7d38ak/test-app.tomaev-maksim.ru./A][0m
[0m[1myandex_dns_recordset.kas: Refreshing state... [id=dns39tr9k9127j7d38ak/kas.tomaev-maksim.ru./A][0m
[0m[1myandex_dns_recordset.grafana: Refreshing state... [id=dns39tr9k9127j7d38ak/grafana.tomaev-maksim.ru./A][0m
[0m[1myandex_dns_recordset.minio: Refreshing state... [id=dns39tr9k9127j7d38ak/minio.tomaev-maksim.ru./A][0m
[0m[1myandex_dns_recordset.gitlab: Refreshing state... [id=dns39tr9k9127j7d38ak/gitlab.tomaev-maksim.ru./A][0m
[0m[1mlocal_file.project: Refreshing state... [id=cf6b65792a288b744ff11709b16e6b69aae4f689][0m
[0m[1mlocal_file.addons: Refreshing state... [id=385c144e7a968496a47b5b971a467b743788f7a1][0m
[0m[1mlocal_file.k8s-cluster: Refreshing state... [id=4c4dd90f2e3f246f677966334d8ea237ed1f2d09][0m

Terraform used the selected providers to generate the following execution
plan. Resource actions are indicated with the following symbols:
  [31m-[0m destroy[0m

Terraform will perform the following actions:

[1m  # local_file.addons[0m will be [1m[31mdestroyed[0m
[0m  [31m-[0m[0m resource "local_file" "addons" {
      [31m-[0m[0m content              = <<-EOT
            ---
            # Kubernetes dashboard
            # RBAC required. see docs/getting-started.md for access details.
            # dashboard_enabled: true
            
            # Helm deployment
            helm_enabled: true
            
            # Registry deployment
            registry_enabled: false
            # registry_namespace: kube-system
            # registry_storage_class: ""
            # registry_disk_size: "10Gi"
            
            # Metrics Server deployment
            metrics_server_enabled: false
            # metrics_server_container_port: 10250
            # metrics_server_kubelet_insecure_tls: true
            # metrics_server_metric_resolution: 15s
            # metrics_server_kubelet_preferred_address_types: "InternalIP,ExternalIP,Hostname"
            # metrics_server_host_network: false
            # metrics_server_replicas: 1
            
            # Rancher Local Path Provisioner
            local_path_provisioner_enabled: true
            # local_path_provisioner_namespace: "local-path-storage"
            # local_path_provisioner_storage_class: "local-path"
            # local_path_provisioner_reclaim_policy: Delete
            # local_path_provisioner_claim_root: /opt/local-path-provisioner/
            # local_path_provisioner_debug: false
            # local_path_provisioner_image_repo: "rancher/local-path-provisioner"
            # local_path_provisioner_image_tag: "v0.0.24"
            # local_path_provisioner_helper_image_repo: "busybox"
            # local_path_provisioner_helper_image_tag: "latest"
            
            # Local volume provisioner deployment
            local_volume_provisioner_enabled: true
            # local_volume_provisioner_namespace: kube-system
            # local_volume_provisioner_nodelabels:
            #   - kubernetes.io/hostname
            #   - topology.kubernetes.io/region
            #   - topology.kubernetes.io/zone
            # local_volume_provisioner_storage_classes:
            #   local-storage:
            #     host_dir: /mnt/disks
            #     mount_dir: /mnt/disks
            #     volume_mode: Filesystem
            #     fs_type: ext4
            #   fast-disks:
            #     host_dir: /mnt/fast-disks
            #     mount_dir: /mnt/fast-disks
            #     block_cleaner_command:
            #       - "/scripts/shred.sh"
            #       - "2"
            #     volume_mode: Filesystem
            #     fs_type: ext4
            # local_volume_provisioner_tolerations:
            #   - effect: NoSchedule
            #     operator: Exists
            
            # CSI Volume Snapshot Controller deployment, set this to true if your CSI is able to manage snapshots
            # currently, setting cinder_csi_enabled=true would automatically enable the snapshot controller
            # Longhorn is an extenal CSI that would also require setting this to true but it is not included in kubespray
            # csi_snapshot_controller_enabled: false
            # csi snapshot namespace
            # snapshot_controller_namespace: kube-system
            
            # CephFS provisioner deployment
            cephfs_provisioner_enabled: false
            # cephfs_provisioner_namespace: "cephfs-provisioner"
            # cephfs_provisioner_cluster: ceph
            # cephfs_provisioner_monitors: "172.24.0.1:6789,172.24.0.2:6789,172.24.0.3:6789"
            # cephfs_provisioner_admin_id: admin
            # cephfs_provisioner_secret: secret
            # cephfs_provisioner_storage_class: cephfs
            # cephfs_provisioner_reclaim_policy: Delete
            # cephfs_provisioner_claim_root: /volumes
            # cephfs_provisioner_deterministic_names: true
            
            # RBD provisioner deployment
            rbd_provisioner_enabled: false
            # rbd_provisioner_namespace: rbd-provisioner
            # rbd_provisioner_replicas: 2
            # rbd_provisioner_monitors: "172.24.0.1:6789,172.24.0.2:6789,172.24.0.3:6789"
            # rbd_provisioner_pool: kube
            # rbd_provisioner_admin_id: admin
            # rbd_provisioner_secret_name: ceph-secret-admin
            # rbd_provisioner_secret: ceph-key-admin
            # rbd_provisioner_user_id: kube
            # rbd_provisioner_user_secret_name: ceph-secret-user
            # rbd_provisioner_user_secret: ceph-key-user
            # rbd_provisioner_user_secret_namespace: rbd-provisioner
            # rbd_provisioner_fs_type: ext4
            # rbd_provisioner_image_format: "2"
            # rbd_provisioner_image_features: layering
            # rbd_provisioner_storage_class: rbd
            # rbd_provisioner_reclaim_policy: Delete
            
            # Nginx ingress controller deployment
            ingress_nginx_enabled: true
            # ingress_nginx_host_network: false
            ingress_publish_status_address: "84.201.133.127"
            # ingress_nginx_nodeselector:
            #   kubernetes.io/os: "linux"
            # ingress_nginx_tolerations:
            #   - key: "node-role.kubernetes.io/master"
            #     operator: "Equal"
            #     value: ""
            #     effect: "NoSchedule"
            #   - key: "node-role.kubernetes.io/control-plane"
            #     operator: "Equal"
            #     value: ""
            #     effect: "NoSchedule"
            # ingress_nginx_namespace: "ingress-nginx"
            # ingress_nginx_insecure_port: 80
            # ingress_nginx_secure_port: 443
            # ingress_nginx_configmap:
            #   map-hash-bucket-size: "128"
            #   ssl-protocols: "TLSv1.2 TLSv1.3"
            # ingress_nginx_configmap_tcp_services:
            #   9000: "default/example-go:8080"
            # ingress_nginx_configmap_udp_services:
            #   53: "kube-system/coredns:53"
            # ingress_nginx_extra_args:
            #   - --default-ssl-certificate=default/foo-tls
            # ingress_nginx_termination_grace_period_seconds: 300
            # ingress_nginx_class: nginx
            # ingress_nginx_without_class: true
            # ingress_nginx_default: false
            
            # ALB ingress controller deployment
            ingress_alb_enabled: false
            # alb_ingress_aws_region: "us-east-1"
            # alb_ingress_restrict_scheme: "false"
            # Enables logging on all outbound requests sent to the AWS API.
            # If logging is desired, set to true.
            # alb_ingress_aws_debug: "false"
            
            # Cert manager deployment
            cert_manager_enabled: false
            # cert_manager_namespace: "cert-manager"
            # cert_manager_tolerations:
            #   - key: node-role.kubernetes.io/master
            #     effect: NoSchedule
            #   - key: node-role.kubernetes.io/control-plane
            #     effect: NoSchedule
            # cert_manager_affinity:
            #  nodeAffinity:
            #    preferredDuringSchedulingIgnoredDuringExecution:
            #    - weight: 100
            #      preference:
            #        matchExpressions:
            #        - key: node-role.kubernetes.io/control-plane
            #          operator: In
            #          values:
            #          - ""
            # cert_manager_nodeselector:
            #   kubernetes.io/os: "linux"
            
            # cert_manager_trusted_internal_ca: |
            #   -----BEGIN CERTIFICATE-----
            #   [REPLACE with your CA certificate]
            #   -----END CERTIFICATE-----
            # cert_manager_leader_election_namespace: kube-system
            
            # cert_manager_dns_policy: "ClusterFirst"
            # cert_manager_dns_config:
            #   nameservers:
            #     - "1.1.1.1"
            #     - "8.8.8.8"
            
            # cert_manager_controller_extra_args:
            #   - "--dns01-recursive-nameservers-only=true"
            #   - "--dns01-recursive-nameservers=1.1.1.1:53,8.8.8.8:53"
            
            # MetalLB deployment
            metallb_enabled: false
            metallb_speaker_enabled: "{{ metallb_enabled }}"
            # metallb_speaker_nodeselector:
            #   kubernetes.io/os: "linux"
            # metallb_controller_nodeselector:
            #   kubernetes.io/os: "linux"
            # metallb_speaker_tolerations:
            #   - key: "node-role.kubernetes.io/master"
            #     operator: "Equal"
            #     value: ""
            #     effect: "NoSchedule"
            #   - key: "node-role.kubernetes.io/control-plane"
            #     operator: "Equal"
            #     value: ""
            #     effect: "NoSchedule"
            # metallb_controller_tolerations:
            #   - key: "node-role.kubernetes.io/master"
            #     operator: "Equal"
            #     value: ""
            #     effect: "NoSchedule"
            #   - key: "node-role.kubernetes.io/control-plane"
            #     operator: "Equal"
            #     value: ""
            #     effect: "NoSchedule"
            # metallb_version: v0.13.9
            # metallb_protocol: "layer2"
            # metallb_port: "7472"
            # metallb_memberlist_port: "7946"
            # metallb_config:
            #   address_pools:
            #     primary:
            #       ip_range:
            #         - 10.5.0.0/16
            #       auto_assign: true
            #     pool1:
            #       ip_range:
            #         - 10.6.0.0/16
            #       auto_assign: true
            #     pool2:
            #       ip_range:
            #         - 10.10.0.0/16
            #       auto_assign: true
            #   layer2:
            #     - primary
            #   layer3:
            #     defaults:
            #       peer_port: 179
            #       hold_time: 120s
            #     communities:
            #       vpn-only: "1234:1"
            #       NO_ADVERTISE: "65535:65282"
            #     metallb_peers:
            #         peer1:
            #           peer_address: 10.6.0.1
            #           peer_asn: 64512
            #           my_asn: 4200000000
            #           communities:
            #             - vpn-only
            #           address_pool:
            #             - pool1
            #         peer2:
            #           peer_address: 10.10.0.1
            #           peer_asn: 64513
            #           my_asn: 4200000000
            #           communities:
            #             - NO_ADVERTISE
            #           address_pool:
            #             - pool2
            
            argocd_enabled: false
            # argocd_version: v2.8.0
            # argocd_namespace: argocd
            # Default password:
            #   - https://argo-cd.readthedocs.io/en/stable/getting_started/#4-login-using-the-cli
            #   ---
            #   The initial password is autogenerated and stored in `argocd-initial-admin-secret` in the argocd namespace defined above.
            #   Using the argocd CLI the generated password can be automatically be fetched from the current kubectl context with the command:
            #   argocd admin initial-password -n argocd
            #   ---
            # Use the following var to set admin password
            # argocd_admin_password: "password"
            
            # The plugin manager for kubectl
            krew_enabled: false
            krew_root_dir: "/usr/local/krew"
        EOT [90m-> null[0m[0m
      [31m-[0m[0m content_base64sha256 = "x6fzl20ibUs/BWcB0k/QuTyejDF8moZAbY/da0+Uh00=" [90m-> null[0m[0m
      [31m-[0m[0m content_base64sha512 = "2CrvZzCwtAGSKLsChzi2ns10/RUD6Pw7OUEcbHaKZ9eDOzNCA1Z2o3e0Ctxjf0Yev9ZB0U/SW9fEWtwRKoKdRA==" [90m-> null[0m[0m
      [31m-[0m[0m content_md5          = "341727a13e214eb13364c244eab7c07d" [90m-> null[0m[0m
      [31m-[0m[0m content_sha1         = "385c144e7a968496a47b5b971a467b743788f7a1" [90m-> null[0m[0m
      [31m-[0m[0m content_sha256       = "c7a7f3976d226d4b3f056701d24fd0b93c9e8c317c9a86406d8fdd6b4f94874d" [90m-> null[0m[0m
      [31m-[0m[0m content_sha512       = "d82aef6730b0b4019228bb028738b69ecd74fd1503e8fc3b39411c6c768a67d7833b3342035676a377b40adc637f461ebfd641d14fd25bd7c45adc112a829d44" [90m-> null[0m[0m
      [31m-[0m[0m directory_permission = "0777" [90m-> null[0m[0m
      [31m-[0m[0m file_permission      = "0777" [90m-> null[0m[0m
      [31m-[0m[0m filename             = "inventory/addons.yml" [90m-> null[0m[0m
      [31m-[0m[0m id                   = "385c144e7a968496a47b5b971a467b743788f7a1" [90m-> null[0m[0m
    }

[1m  # local_file.k8s-cluster[0m will be [1m[31mdestroyed[0m
[0m  [31m-[0m[0m resource "local_file" "k8s-cluster" {
      [31m-[0m[0m content              = <<-EOT
            ---
            # Kubernetes configuration dirs and system namespace.
            # Those are where all the additional config stuff goes
            # the kubernetes normally puts in /srv/kubernetes.
            # This puts them in a sane location and namespace.
            # Editing those values will almost surely break something.
            kube_config_dir: /etc/kubernetes
            kube_script_dir: "{{ bin_dir }}/kubernetes-scripts"
            kube_manifest_dir: "{{ kube_config_dir }}/manifests"
            
            # This is where all the cert scripts and certs will be located
            kube_cert_dir: "{{ kube_config_dir }}/ssl"
            
            # This is where all of the bearer tokens will be stored
            kube_token_dir: "{{ kube_config_dir }}/tokens"
            
            kube_api_anonymous_auth: true
            
            ## Change this to use another Kubernetes version, e.g. a current beta release
            kube_version: v1.27.7
            
            # Where the binaries will be downloaded.
            # Note: ensure that you've enough disk space (about 1G)
            local_release_dir: "/tmp/releases"
            # Random shifts for retrying failed ops like pushing/downloading
            retry_stagger: 5
            
            # This is the user that owns tha cluster installation.
            kube_owner: kube
            
            # This is the group that the cert creation scripts chgrp the
            # cert files to. Not really changeable...
            kube_cert_group: kube-cert
            
            # Cluster Loglevel configuration
            kube_log_level: 2
            
            # Directory where credentials will be stored
            credentials_dir: "{{ inventory_dir }}/credentials"
            
            ## It is possible to activate / deactivate selected authentication methods (oidc, static token auth)
            # kube_oidc_auth: false
            # kube_token_auth: false
            
            
            ## Variables for OpenID Connect Configuration https://kubernetes.io/docs/admin/authentication/
            ## To use OpenID you have to deploy additional an OpenID Provider (e.g Dex, Keycloak, ...)
            
            # kube_oidc_url: https:// ...
            # kube_oidc_client_id: kubernetes
            ## Optional settings for OIDC
            # kube_oidc_ca_file: "{{ kube_cert_dir }}/ca.pem"
            # kube_oidc_username_claim: sub
            # kube_oidc_username_prefix: 'oidc:'
            # kube_oidc_groups_claim: groups
            # kube_oidc_groups_prefix: 'oidc:'
            
            ## Variables to control webhook authn/authz
            # kube_webhook_token_auth: false
            # kube_webhook_token_auth_url: https://...
            # kube_webhook_token_auth_url_skip_tls_verify: false
            
            ## For webhook authorization, authorization_modes must include Webhook
            # kube_webhook_authorization: false
            # kube_webhook_authorization_url: https://...
            # kube_webhook_authorization_url_skip_tls_verify: false
            
            # Choose network plugin (cilium, calico, kube-ovn, weave or flannel. Use cni for generic cni plugin)
            # Can also be set to 'cloud', which lets the cloud provider setup appropriate routing
            kube_network_plugin: calico
            
            # Setting multi_networking to true will install Multus: https://github.com/k8snetworkplumbingwg/multus-cni
            kube_network_plugin_multus: false
            
            # Kubernetes internal network for services, unused block of space.
            kube_service_addresses: 10.233.0.0/18
            
            # internal network. When used, it will assign IP
            # addresses from this range to individual pods.
            # This network must be unused in your network infrastructure!
            kube_pods_subnet: 10.233.64.0/18
            
            # internal network node size allocation (optional). This is the size allocated
            # to each node for pod IP address allocation. Note that the number of pods per node is
            # also limited by the kubelet_max_pods variable which defaults to 110.
            #
            # Example:
            # Up to 64 nodes and up to 254 or kubelet_max_pods (the lowest of the two) pods per node:
            #  - kube_pods_subnet: 10.233.64.0/18
            #  - kube_network_node_prefix: 24
            #  - kubelet_max_pods: 110
            #
            # Example:
            # Up to 128 nodes and up to 126 or kubelet_max_pods (the lowest of the two) pods per node:
            #  - kube_pods_subnet: 10.233.64.0/18
            #  - kube_network_node_prefix: 25
            #  - kubelet_max_pods: 110
            kube_network_node_prefix: 24
            
            # Configure Dual Stack networking (i.e. both IPv4 and IPv6)
            enable_dual_stack_networks: false
            
            # Kubernetes internal network for IPv6 services, unused block of space.
            # This is only used if enable_dual_stack_networks is set to true
            # This provides 4096 IPv6 IPs
            kube_service_addresses_ipv6: fd85:ee78:d8a6:8607::1000/116
            
            # Internal network. When used, it will assign IPv6 addresses from this range to individual pods.
            # This network must not already be in your network infrastructure!
            # This is only used if enable_dual_stack_networks is set to true.
            # This provides room for 256 nodes with 254 pods per node.
            kube_pods_subnet_ipv6: fd85:ee78:d8a6:8607::1:0000/112
            
            # IPv6 subnet size allocated to each for pods.
            # This is only used if enable_dual_stack_networks is set to true
            # This provides room for 254 pods per node.
            kube_network_node_prefix_ipv6: 120
            
            # The port the API Server will be listening on.
            kube_apiserver_ip: "{{ kube_service_addresses | ipaddr('net') | ipaddr(1) | ipaddr('address') }}"
            kube_apiserver_port: 6443  # (https)
            
            # Kube-proxy proxyMode configuration.
            # Can be ipvs, iptables
            kube_proxy_mode: ipvs
            
            # configure arp_ignore and arp_announce to avoid answering ARP queries from kube-ipvs0 interface
            # must be set to true for MetalLB, kube-vip(ARP enabled) to work
            kube_proxy_strict_arp: false
            
            # A string slice of values which specify the addresses to use for NodePorts.
            # Values may be valid IP blocks (e.g. 1.2.3.0/24, 1.2.3.4/32).
            # The default empty string slice ([]) means to use all local addresses.
            # kube_proxy_nodeport_addresses_cidr is retained for legacy config
            kube_proxy_nodeport_addresses: >-
              {%- if kube_proxy_nodeport_addresses_cidr is defined -%}
              [{{ kube_proxy_nodeport_addresses_cidr }}]
              {%- else -%}
              []
              {%- endif -%}
            
            # If non-empty, will use this string as identification instead of the actual hostname
            # kube_override_hostname: >-
            #   {%- if cloud_provider is defined and cloud_provider in ['aws'] -%}
            #   {%- else -%}
            #   {{ inventory_hostname }}
            #   {%- endif -%}
            
            ## Encrypting Secret Data at Rest
            kube_encrypt_secret_data: false
            
            # Graceful Node Shutdown (Kubernetes >= 1.21.0), see https://kubernetes.io/blog/2021/04/21/graceful-node-shutdown-beta/
            # kubelet_shutdown_grace_period had to be greater than kubelet_shutdown_grace_period_critical_pods to allow
            # non-critical podsa to also terminate gracefully
            # kubelet_shutdown_grace_period: 60s
            # kubelet_shutdown_grace_period_critical_pods: 20s
            
            # DNS configuration.
            # Kubernetes cluster name, also will be used as DNS domain
            cluster_name: cluster.local
            # Subdomains of DNS domain to be resolved via /etc/resolv.conf for hostnet pods
            ndots: 2
            # dns_timeout: 2
            # dns_attempts: 2
            # Custom search domains to be added in addition to the default cluster search domains
            # searchdomains:
            #   - svc.{{ cluster_name }}
            #   - default.svc.{{ cluster_name }}
            # Remove default cluster search domains (``default.svc.{{ dns_domain }}, svc.{{ dns_domain }}``).
            # remove_default_searchdomains: false
            # Can be coredns, coredns_dual, manual or none
            dns_mode: coredns
            # Set manual server if using a custom cluster DNS server
            # manual_dns_server: 10.x.x.x
            # Enable nodelocal dns cache
            enable_nodelocaldns: true
            enable_nodelocaldns_secondary: false
            nodelocaldns_ip: 169.254.25.10
            nodelocaldns_health_port: 9254
            nodelocaldns_second_health_port: 9256
            nodelocaldns_bind_metrics_host_ip: false
            nodelocaldns_secondary_skew_seconds: 5
            # nodelocaldns_external_zones:
            # - zones:
            #   - example.com
            #   - example.io:1053
            #   nameservers:
            #   - 1.1.1.1
            #   - 2.2.2.2
            #   cache: 5
            # - zones:
            #   - https://mycompany.local:4453
            #   nameservers:
            #   - 192.168.0.53
            #   cache: 0
            # - zones:
            #   - mydomain.tld
            #   nameservers:
            #   - 10.233.0.3
            #   cache: 5
            #   rewrite:
            #   - name website.tld website.namespace.svc.cluster.local
            # Enable k8s_external plugin for CoreDNS
            enable_coredns_k8s_external: false
            coredns_k8s_external_zone: k8s_external.local
            # Enable endpoint_pod_names option for kubernetes plugin
            enable_coredns_k8s_endpoint_pod_names: false
            # Set forward options for upstream DNS servers in coredns (and nodelocaldns) config
            # dns_upstream_forward_extra_opts:
            #   policy: sequential
            # Apply extra options to coredns kubernetes plugin
            # coredns_kubernetes_extra_opts:
            #   - 'fallthrough example.local'
            # Forward extra domains to the coredns kubernetes plugin
            # coredns_kubernetes_extra_domains: ''
            
            # Can be docker_dns, host_resolvconf or none
            resolvconf_mode: host_resolvconf
            # Deploy netchecker app to verify DNS resolve as an HTTP service
            deploy_netchecker: false
            # Ip address of the kubernetes skydns service
            skydns_server: "{{ kube_service_addresses | ipaddr('net') | ipaddr(3) | ipaddr('address') }}"
            skydns_server_secondary: "{{ kube_service_addresses | ipaddr('net') | ipaddr(4) | ipaddr('address') }}"
            dns_domain: "{{ cluster_name }}"
            
            ## Container runtime
            ## docker for docker, crio for cri-o and containerd for containerd.
            ## Default: containerd
            container_manager: containerd
            
            # Additional container runtimes
            kata_containers_enabled: false
            
            kubeadm_certificate_key: "{{ lookup('password', credentials_dir + '/kubeadm_certificate_key.creds length=64 chars=hexdigits') | lower }}"
            
            # K8s image pull policy (imagePullPolicy)
            k8s_image_pull_policy: IfNotPresent
            
            # audit log for kubernetes
            kubernetes_audit: false
            
            # define kubelet config dir for dynamic kubelet
            # kubelet_config_dir:
            default_kubelet_config_dir: "{{ kube_config_dir }}/dynamic_kubelet_dir"
            
            # pod security policy (RBAC must be enabled either by having 'RBAC' in authorization_modes or kubeadm enabled)
            podsecuritypolicy_enabled: false
            
            # Custom PodSecurityPolicySpec for restricted policy
            # podsecuritypolicy_restricted_spec: {}
            
            # Custom PodSecurityPolicySpec for privileged policy
            # podsecuritypolicy_privileged_spec: {}
            
            # Make a copy of kubeconfig on the host that runs Ansible in {{ inventory_dir }}/artifacts
            # kubeconfig_localhost: false
            # Use ansible_host as external api ip when copying over kubeconfig.
            # kubeconfig_localhost_ansible_host: false
            # Download kubectl onto the host that runs Ansible in {{ bin_dir }}
            # kubectl_localhost: false
            
            # A comma separated list of levels of node allocatable enforcement to be enforced by kubelet.
            # Acceptable options are 'pods', 'system-reserved', 'kube-reserved' and ''. Default is "".
            # kubelet_enforce_node_allocatable: pods
            
            ## Set runtime and kubelet cgroups when using systemd as cgroup driver (default)
            # kubelet_runtime_cgroups: "/{{ kube_service_cgroups }}/{{ container_manager }}.service"
            # kubelet_kubelet_cgroups: "/{{ kube_service_cgroups }}/kubelet.service"
            
            ## Set runtime and kubelet cgroups when using cgroupfs as cgroup driver
            # kubelet_runtime_cgroups_cgroupfs: "/system.slice/{{ container_manager }}.service"
            # kubelet_kubelet_cgroups_cgroupfs: "/system.slice/kubelet.service"
            
            # Optionally reserve this space for kube daemons.
            # kube_reserved: false
            ## Uncomment to override default values
            ## The following two items need to be set when kube_reserved is true
            # kube_reserved_cgroups_for_service_slice: kube.slice
            # kube_reserved_cgroups: "/{{ kube_reserved_cgroups_for_service_slice }}"
            # kube_memory_reserved: 256Mi
            # kube_cpu_reserved: 100m
            # kube_ephemeral_storage_reserved: 2Gi
            # kube_pid_reserved: "1000"
            # Reservation for master hosts
            # kube_master_memory_reserved: 512Mi
            # kube_master_cpu_reserved: 200m
            # kube_master_ephemeral_storage_reserved: 2Gi
            # kube_master_pid_reserved: "1000"
            
            ## Optionally reserve resources for OS system daemons.
            # system_reserved: true
            ## Uncomment to override default values
            ## The following two items need to be set when system_reserved is true
            # system_reserved_cgroups_for_service_slice: system.slice
            # system_reserved_cgroups: "/{{ system_reserved_cgroups_for_service_slice }}"
            # system_memory_reserved: 512Mi
            # system_cpu_reserved: 500m
            # system_ephemeral_storage_reserved: 2Gi
            ## Reservation for master hosts
            # system_master_memory_reserved: 256Mi
            # system_master_cpu_reserved: 250m
            # system_master_ephemeral_storage_reserved: 2Gi
            
            ## Eviction Thresholds to avoid system OOMs
            # https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#eviction-thresholds
            # eviction_hard: {}
            # eviction_hard_control_plane: {}
            
            # An alternative flexvolume plugin directory
            # kubelet_flexvolumes_plugins_dir: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
            
            ## Supplementary addresses that can be added in kubernetes ssl keys.
            ## That can be useful for example to setup a keepalived virtual IP
            supplementary_addresses_in_ssl_keys: [84.201.133.127]
            
            ## Running on top of openstack vms with cinder enabled may lead to unschedulable pods due to NoVolumeZoneConflict restriction in kube-scheduler.
            ## See https://github.com/kubernetes-sigs/kubespray/issues/2141
            ## Set this variable to true to get rid of this issue
            volume_cross_zone_attachment: false
            ## Add Persistent Volumes Storage Class for corresponding cloud provider (supported: in-tree OpenStack, Cinder CSI,
            ## AWS EBS CSI, Azure Disk CSI, GCP Persistent Disk CSI)
            persistent_volumes_enabled: true
            
            ## Container Engine Acceleration
            ## Enable container acceleration feature, for example use gpu acceleration in containers
            # nvidia_accelerator_enabled: true
            ## Nvidia GPU driver install. Install will by done by a (init) pod running as a daemonset.
            ## Important: if you use Ubuntu then you should set in all.yml 'docker_storage_options: -s overlay2'
            ## Array with nvida_gpu_nodes, leave empty or comment if you don't want to install drivers.
            ## Labels and taints won't be set to nodes if they are not in the array.
            # nvidia_gpu_nodes:
            #   - kube-gpu-001
            # nvidia_driver_version: "384.111"
            ## flavor can be tesla or gtx
            # nvidia_gpu_flavor: gtx
            ## NVIDIA driver installer images. Change them if you have trouble accessing gcr.io.
            # nvidia_driver_install_centos_container: atzedevries/nvidia-centos-driver-installer:2
            # nvidia_driver_install_ubuntu_container: gcr.io/google-containers/ubuntu-nvidia-driver-installer@sha256:7df76a0f0a17294e86f691c81de6bbb7c04a1b4b3d4ea4e7e2cccdc42e1f6d63
            ## NVIDIA GPU device plugin image.
            # nvidia_gpu_device_plugin_container: "registry.k8s.io/nvidia-gpu-device-plugin@sha256:0842734032018be107fa2490c98156992911e3e1f2a21e059ff0105b07dd8e9e"
            
            ## Support tls min version, Possible values: VersionTLS10, VersionTLS11, VersionTLS12, VersionTLS13.
            # tls_min_version: ""
            
            ## Support tls cipher suites.
            # tls_cipher_suites: {}
            #   - TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA
            #   - TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256
            #   - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256
            #   - TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA
            #   - TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384
            #   - TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305
            #   - TLS_ECDHE_ECDSA_WITH_RC4_128_SHA
            #   - TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA
            #   - TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA
            #   - TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
            #   - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
            #   - TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA
            #   - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
            #   - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
            #   - TLS_ECDHE_RSA_WITH_RC4_128_SHA
            #   - TLS_RSA_WITH_3DES_EDE_CBC_SHA
            #   - TLS_RSA_WITH_AES_128_CBC_SHA
            #   - TLS_RSA_WITH_AES_128_CBC_SHA256
            #   - TLS_RSA_WITH_AES_128_GCM_SHA256
            #   - TLS_RSA_WITH_AES_256_CBC_SHA
            #   - TLS_RSA_WITH_AES_256_GCM_SHA384
            #   - TLS_RSA_WITH_RC4_128_SHA
            
            ## Amount of time to retain events. (default 1h0m0s)
            event_ttl_duration: "1h0m0s"
            
            ## Automatically renew K8S control plane certificates on first Monday of each month
            auto_renew_certificates: false
            # First Monday of each month
            # auto_renew_certificates_systemd_calendar: "Mon *-*-1,2,3,4,5,6,7 03:{{ groups['kube_control_plane'].index(inventory_hostname) }}0:00"
            
            # kubeadm patches path
            kubeadm_patches:
              enabled: false
              source_dir: "{{ inventory_dir }}/patches"
              dest_dir: "{{ kube_config_dir }}/patches"
        EOT [90m-> null[0m[0m
      [31m-[0m[0m content_base64sha256 = "IIoIWSepqThXd4UdD4j3Hhv9djvmCcyzkD1TktwbhBI=" [90m-> null[0m[0m
      [31m-[0m[0m content_base64sha512 = "uyVAVxipHimblkY/yMVbZpqMzvhH2e0LDoCud9PtFSxfZ9aOoaXbl94uO/cKOigi3n/8nefbpOdaQR7dnIBnRQ==" [90m-> null[0m[0m
      [31m-[0m[0m content_md5          = "f4bda20c524262a7ce8489c9256a6f9e" [90m-> null[0m[0m
      [31m-[0m[0m content_sha1         = "4c4dd90f2e3f246f677966334d8ea237ed1f2d09" [90m-> null[0m[0m
      [31m-[0m[0m content_sha256       = "208a085927a9a9385777851d0f88f71e1bfd763be609ccb3903d5392dc1b8412" [90m-> null[0m[0m
      [31m-[0m[0m content_sha512       = "bb25405718a91e299b96463fc8c55b669a8ccef847d9ed0b0e80ae77d3ed152c5f67d68ea1a5db97de2e3bf70a3a2822de7ffc9de7dba4e75a411edd9c806745" [90m-> null[0m[0m
      [31m-[0m[0m directory_permission = "0777" [90m-> null[0m[0m
      [31m-[0m[0m file_permission      = "0777" [90m-> null[0m[0m
      [31m-[0m[0m filename             = "inventory/k8s-cluster.yml" [90m-> null[0m[0m
      [31m-[0m[0m id                   = "4c4dd90f2e3f246f677966334d8ea237ed1f2d09" [90m-> null[0m[0m
    }

[1m  # local_file.project[0m will be [1m[31mdestroyed[0m
[0m  [31m-[0m[0m resource "local_file" "project" {
      [31m-[0m[0m content              = <<-EOT
            all:
              hosts:
                node1:
                  ansible_host: 84.201.133.127
                  ip: 10.5.0.6
                  access_ip: 10.5.0.6
                node2:
                  ansible_host: 84.201.177.170
                  ip: 10.6.0.7
                  access_ip: 10.6.0.7
                node3:
                  ansible_host: 158.160.140.224
                  ip: 10.7.0.33
                  access_ip: 10.7.0.33
              children:
                kube_control_plane:
                  hosts:
                    node1:
                kube_node:
                  hosts:
                    node1:
                    node2:
                    node3:
                etcd:
                  hosts:
                    node1:
                k8s_cluster:
                  children:
                    kube_control_plane:
                    kube_node:
                calico_rr:
                  hosts: {}
        EOT [90m-> null[0m[0m
      [31m-[0m[0m content_base64sha256 = "nwrzIFCzVuCqWOsV5vi3BBff4PmUwFM0a4jH1wgkUVo=" [90m-> null[0m[0m
      [31m-[0m[0m content_base64sha512 = "/Wv/0YZi1fU7Fo7QvSaViY7VbpXQvFWUwWQN1gBmjV1NviNu5bgq7rwlFN4U68g+oa7ZjH1rws4RQI2hONg+fQ==" [90m-> null[0m[0m
      [31m-[0m[0m content_md5          = "35634d3b769a5039689d8d10c794a536" [90m-> null[0m[0m
      [31m-[0m[0m content_sha1         = "cf6b65792a288b744ff11709b16e6b69aae4f689" [90m-> null[0m[0m
      [31m-[0m[0m content_sha256       = "9f0af32050b356e0aa58eb15e6f8b70417dfe0f994c053346b88c7d70824515a" [90m-> null[0m[0m
      [31m-[0m[0m content_sha512       = "fd6bffd18662d5f53b168ed0bd2695898ed56e95d0bc5594c1640dd600668d5d4dbe236ee5b82aeebc2514de14ebc83ea1aed98c7d6bc2ce11408da138d83e7d" [90m-> null[0m[0m
      [31m-[0m[0m directory_permission = "0777" [90m-> null[0m[0m
      [31m-[0m[0m file_permission      = "0777" [90m-> null[0m[0m
      [31m-[0m[0m filename             = "inventory/hosts.yaml" [90m-> null[0m[0m
      [31m-[0m[0m id                   = "cf6b65792a288b744ff11709b16e6b69aae4f689" [90m-> null[0m[0m
    }

[1m  # yandex_compute_image.ubuntu-test[0m will be [1m[31mdestroyed[0m
[0m  [31m-[0m[0m resource "yandex_compute_image" "ubuntu-test" {
      [31m-[0m[0m created_at    = "2024-02-05T17:17:31Z" [90m-> null[0m[0m
      [31m-[0m[0m folder_id     = "b1gb1aal3vgk7p7nr6nd" [90m-> null[0m[0m
      [31m-[0m[0m id            = "fd8vjq5ab5g28bj5bojc" [90m-> null[0m[0m
      [31m-[0m[0m labels        = {} [90m-> null[0m[0m
      [31m-[0m[0m min_disk_size = 8 [90m-> null[0m[0m
      [31m-[0m[0m pooled        = false [90m-> null[0m[0m
      [31m-[0m[0m product_ids   = [
          [31m-[0m[0m "f2e9g2vupcv6ihur8v1b",
        ] [90m-> null[0m[0m
      [31m-[0m[0m size          = 7 [90m-> null[0m[0m
      [31m-[0m[0m source_family = "ubuntu-2204-lts" [90m-> null[0m[0m
      [31m-[0m[0m status        = "ready" [90m-> null[0m[0m
    }

[1m  # yandex_compute_instance_group.k8s-node[0m will be [1m[31mdestroyed[0m
[0m  [31m-[0m[0m resource "yandex_compute_instance_group" "k8s-node" {
      [31m-[0m[0m created_at          = "2024-02-05T17:18:46Z" [90m-> null[0m[0m
      [31m-[0m[0m deletion_protection = false [90m-> null[0m[0m
      [31m-[0m[0m folder_id           = "b1gb1aal3vgk7p7nr6nd" [90m-> null[0m[0m
      [31m-[0m[0m id                  = "cl1eq71t61q4uqudcs0m" [90m-> null[0m[0m
      [31m-[0m[0m instances           = [
          [31m-[0m[0m {
              [31m-[0m[0m fqdn              = "worker-node-1.ru-central1.internal"
              [31m-[0m[0m instance_id       = "fhmh6et7bjb9124vt8if"
              [31m-[0m[0m instance_tag      = ""
              [31m-[0m[0m name              = "worker-node-1"
              [31m-[0m[0m network_interface = [
                  [31m-[0m[0m {
                      [31m-[0m[0m index          = 0
                      [31m-[0m[0m ip_address     = "10.5.0.6"
                      [31m-[0m[0m ipv4           = true
                      [31m-[0m[0m ipv6           = false
                      [31m-[0m[0m ipv6_address   = ""
                      [31m-[0m[0m mac_address    = "d0:0d:11:33:ba:75"
                      [31m-[0m[0m nat            = true
                      [31m-[0m[0m nat_ip_address = "84.201.133.127"
                      [31m-[0m[0m nat_ip_version = "IPV4"
                      [31m-[0m[0m subnet_id      = "e9b4dh3mco90q2druajv"
                    },
                ]
              [31m-[0m[0m status            = "RUNNING_ACTUAL"
              [31m-[0m[0m status_changed_at = "2024-02-05T17:20:24Z"
              [31m-[0m[0m status_message    = ""
              [31m-[0m[0m zone_id           = "ru-central1-a"
            },
          [31m-[0m[0m {
              [31m-[0m[0m fqdn              = "worker-node-2.ru-central1.internal"
              [31m-[0m[0m instance_id       = "epdhcqdclbip07k2ugi8"
              [31m-[0m[0m instance_tag      = ""
              [31m-[0m[0m name              = "worker-node-2"
              [31m-[0m[0m network_interface = [
                  [31m-[0m[0m {
                      [31m-[0m[0m index          = 0
                      [31m-[0m[0m ip_address     = "10.6.0.7"
                      [31m-[0m[0m ipv4           = true
                      [31m-[0m[0m ipv6           = false
                      [31m-[0m[0m ipv6_address   = ""
                      [31m-[0m[0m mac_address    = "d0:0d:11:66:9a:ca"
                      [31m-[0m[0m nat            = true
                      [31m-[0m[0m nat_ip_address = "84.201.177.170"
                      [31m-[0m[0m nat_ip_version = "IPV4"
                      [31m-[0m[0m subnet_id      = "e2l8kesa74sp6dcqgib1"
                    },
                ]
              [31m-[0m[0m status            = "RUNNING_ACTUAL"
              [31m-[0m[0m status_changed_at = "2024-02-05T17:20:33Z"
              [31m-[0m[0m status_message    = ""
              [31m-[0m[0m zone_id           = "ru-central1-b"
            },
          [31m-[0m[0m {
              [31m-[0m[0m fqdn              = "worker-node-3.ru-central1.internal"
              [31m-[0m[0m instance_id       = "fv492gohgr0vbvf9jsnu"
              [31m-[0m[0m instance_tag      = ""
              [31m-[0m[0m name              = "worker-node-3"
              [31m-[0m[0m network_interface = [
                  [31m-[0m[0m {
                      [31m-[0m[0m index          = 0
                      [31m-[0m[0m ip_address     = "10.7.0.33"
                      [31m-[0m[0m ipv4           = true
                      [31m-[0m[0m ipv6           = false
                      [31m-[0m[0m ipv6_address   = ""
                      [31m-[0m[0m mac_address    = "d0:0d:91:43:11:86"
                      [31m-[0m[0m nat            = true
                      [31m-[0m[0m nat_ip_address = "158.160.140.224"
                      [31m-[0m[0m nat_ip_version = "IPV4"
                      [31m-[0m[0m subnet_id      = "fl84j1g960uevkr0vq2g"
                    },
                ]
              [31m-[0m[0m status            = "RUNNING_ACTUAL"
              [31m-[0m[0m status_changed_at = "2024-02-05T17:19:54Z"
              [31m-[0m[0m status_message    = ""
              [31m-[0m[0m zone_id           = "ru-central1-d"
            },
        ] [90m-> null[0m[0m
      [31m-[0m[0m labels              = {} [90m-> null[0m[0m
      [31m-[0m[0m name                = "k8s-node" [90m-> null[0m[0m
      [31m-[0m[0m service_account_id  = "ajep7io5sk5fidb8vvii" [90m-> null[0m[0m
      [31m-[0m[0m status              = "ACTIVE" [90m-> null[0m[0m
      [31m-[0m[0m variables           = {} [90m-> null[0m[0m

      [31m-[0m[0m allocation_policy {
          [31m-[0m[0m zones = [
              [31m-[0m[0m "ru-central1-a",
              [31m-[0m[0m "ru-central1-b",
              [31m-[0m[0m "ru-central1-d",
            ] [90m-> null[0m[0m
        }

      [31m-[0m[0m deploy_policy {
          [31m-[0m[0m max_creating     = 3 [90m-> null[0m[0m
          [31m-[0m[0m max_deleting     = 3 [90m-> null[0m[0m
          [31m-[0m[0m max_expansion    = 3 [90m-> null[0m[0m
          [31m-[0m[0m max_unavailable  = 3 [90m-> null[0m[0m
          [31m-[0m[0m startup_duration = 0 [90m-> null[0m[0m
          [31m-[0m[0m strategy         = "proactive" [90m-> null[0m[0m
        }

      [31m-[0m[0m instance_template {
          [31m-[0m[0m labels      = {} [90m-> null[0m[0m
          [31m-[0m[0m metadata    = {
              [31m-[0m[0m "user-data" = <<-EOT
                    #cloud-config
                    users:
                      - name: ubuntu
                        groups: sudo
                        shell: /bin/bash
                        sudo: ['ALL=(ALL) NOPASSWD:ALL']
                        ssh-authorized-keys:
                          - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAILzsVjG95NO8uNUlNLhJBXzg75lhffQcZpaqRpchSglS tomaevmax@google.com
                EOT
            } [90m-> null[0m[0m
          [31m-[0m[0m name        = "worker-node-{instance.index}" [90m-> null[0m[0m
          [31m-[0m[0m platform_id = "standard-v2" [90m-> null[0m[0m

          [31m-[0m[0m boot_disk {
              [31m-[0m[0m mode = "READ_WRITE" [90m-> null[0m[0m

              [31m-[0m[0m initialize_params {
                  [31m-[0m[0m image_id = "fd8vjq5ab5g28bj5bojc" [90m-> null[0m[0m
                  [31m-[0m[0m size     = 60 [90m-> null[0m[0m
                  [31m-[0m[0m type     = "network-hdd" [90m-> null[0m[0m
                }
            }

          [31m-[0m[0m network_interface {
              [31m-[0m[0m ipv4               = true [90m-> null[0m[0m
              [31m-[0m[0m ipv6               = false [90m-> null[0m[0m
              [31m-[0m[0m nat                = true [90m-> null[0m[0m
              [31m-[0m[0m network_id         = "enp5ob0gdnqnuq41a4ob" [90m-> null[0m[0m
              [31m-[0m[0m security_group_ids = [] [90m-> null[0m[0m
              [31m-[0m[0m subnet_ids         = [
                  [31m-[0m[0m "e2l8kesa74sp6dcqgib1",
                  [31m-[0m[0m "e9b4dh3mco90q2druajv",
                  [31m-[0m[0m "fl84j1g960uevkr0vq2g",
                ] [90m-> null[0m[0m
            }

          [31m-[0m[0m network_settings {
              [31m-[0m[0m type = "STANDARD" [90m-> null[0m[0m
            }

          [31m-[0m[0m resources {
              [31m-[0m[0m core_fraction = 100 [90m-> null[0m[0m
              [31m-[0m[0m cores         = 4 [90m-> null[0m[0m
              [31m-[0m[0m gpus          = 0 [90m-> null[0m[0m
              [31m-[0m[0m memory        = 16 [90m-> null[0m[0m
            }

          [31m-[0m[0m scheduling_policy {
              [31m-[0m[0m preemptible = true [90m-> null[0m[0m
            }
        }

      [31m-[0m[0m scale_policy {
          [31m-[0m[0m fixed_scale {
              [31m-[0m[0m size = 3 [90m-> null[0m[0m
            }
        }
    }

[1m  # yandex_dns_recordset.gitlab[0m will be [1m[31mdestroyed[0m
[0m  [31m-[0m[0m resource "yandex_dns_recordset" "gitlab" {
      [31m-[0m[0m data    = [
          [31m-[0m[0m "84.201.133.127",
        ] [90m-> null[0m[0m
      [31m-[0m[0m id      = "dns39tr9k9127j7d38ak/gitlab.tomaev-maksim.ru./A" [90m-> null[0m[0m
      [31m-[0m[0m name    = "gitlab.tomaev-maksim.ru." [90m-> null[0m[0m
      [31m-[0m[0m ttl     = 200 [90m-> null[0m[0m
      [31m-[0m[0m type    = "A" [90m-> null[0m[0m
      [31m-[0m[0m zone_id = "dns39tr9k9127j7d38ak" [90m-> null[0m[0m
    }

[1m  # yandex_dns_recordset.grafana[0m will be [1m[31mdestroyed[0m
[0m  [31m-[0m[0m resource "yandex_dns_recordset" "grafana" {
      [31m-[0m[0m data    = [
          [31m-[0m[0m "84.201.133.127",
        ] [90m-> null[0m[0m
      [31m-[0m[0m id      = "dns39tr9k9127j7d38ak/grafana.tomaev-maksim.ru./A" [90m-> null[0m[0m
      [31m-[0m[0m name    = "grafana.tomaev-maksim.ru." [90m-> null[0m[0m
      [31m-[0m[0m ttl     = 200 [90m-> null[0m[0m
      [31m-[0m[0m type    = "A" [90m-> null[0m[0m
      [31m-[0m[0m zone_id = "dns39tr9k9127j7d38ak" [90m-> null[0m[0m
    }

[1m  # yandex_dns_recordset.kas[0m will be [1m[31mdestroyed[0m
[0m  [31m-[0m[0m resource "yandex_dns_recordset" "kas" {
      [31m-[0m[0m data    = [
          [31m-[0m[0m "84.201.133.127",
        ] [90m-> null[0m[0m
      [31m-[0m[0m id      = "dns39tr9k9127j7d38ak/kas.tomaev-maksim.ru./A" [90m-> null[0m[0m
      [31m-[0m[0m name    = "kas.tomaev-maksim.ru." [90m-> null[0m[0m
      [31m-[0m[0m ttl     = 200 [90m-> null[0m[0m
      [31m-[0m[0m type    = "A" [90m-> null[0m[0m
      [31m-[0m[0m zone_id = "dns39tr9k9127j7d38ak" [90m-> null[0m[0m
    }

[1m  # yandex_dns_recordset.minio[0m will be [1m[31mdestroyed[0m
[0m  [31m-[0m[0m resource "yandex_dns_recordset" "minio" {
      [31m-[0m[0m data    = [
          [31m-[0m[0m "84.201.133.127",
        ] [90m-> null[0m[0m
      [31m-[0m[0m id      = "dns39tr9k9127j7d38ak/minio.tomaev-maksim.ru./A" [90m-> null[0m[0m
      [31m-[0m[0m name    = "minio.tomaev-maksim.ru." [90m-> null[0m[0m
      [31m-[0m[0m ttl     = 200 [90m-> null[0m[0m
      [31m-[0m[0m type    = "A" [90m-> null[0m[0m
      [31m-[0m[0m zone_id = "dns39tr9k9127j7d38ak" [90m-> null[0m[0m
    }

[1m  # yandex_dns_recordset.registry[0m will be [1m[31mdestroyed[0m
[0m  [31m-[0m[0m resource "yandex_dns_recordset" "registry" {
      [31m-[0m[0m data    = [
          [31m-[0m[0m "84.201.133.127",
        ] [90m-> null[0m[0m
      [31m-[0m[0m id      = "dns39tr9k9127j7d38ak/registry.tomaev-maksim.ru./A" [90m-> null[0m[0m
      [31m-[0m[0m name    = "registry.tomaev-maksim.ru." [90m-> null[0m[0m
      [31m-[0m[0m ttl     = 200 [90m-> null[0m[0m
      [31m-[0m[0m type    = "A" [90m-> null[0m[0m
      [31m-[0m[0m zone_id = "dns39tr9k9127j7d38ak" [90m-> null[0m[0m
    }

[1m  # yandex_dns_recordset.test-app[0m will be [1m[31mdestroyed[0m
[0m  [31m-[0m[0m resource "yandex_dns_recordset" "test-app" {
      [31m-[0m[0m data    = [
          [31m-[0m[0m "84.201.133.127",
        ] [90m-> null[0m[0m
      [31m-[0m[0m id      = "dns39tr9k9127j7d38ak/test-app.tomaev-maksim.ru./A" [90m-> null[0m[0m
      [31m-[0m[0m name    = "test-app.tomaev-maksim.ru." [90m-> null[0m[0m
      [31m-[0m[0m ttl     = 200 [90m-> null[0m[0m
      [31m-[0m[0m type    = "A" [90m-> null[0m[0m
      [31m-[0m[0m zone_id = "dns39tr9k9127j7d38ak" [90m-> null[0m[0m
    }

[1m  # yandex_dns_zone.diplom[0m will be [1m[31mdestroyed[0m
[0m  [31m-[0m[0m resource "yandex_dns_zone" "diplom" {
      [31m-[0m[0m created_at       = "2024-02-05T17:17:31Z" [90m-> null[0m[0m
      [31m-[0m[0m description      = "домен на время диплома" [90m-> null[0m[0m
      [31m-[0m[0m folder_id        = "b1gb1aal3vgk7p7nr6nd" [90m-> null[0m[0m
      [31m-[0m[0m id               = "dns39tr9k9127j7d38ak" [90m-> null[0m[0m
      [31m-[0m[0m labels           = {} [90m-> null[0m[0m
      [31m-[0m[0m name             = "diplom" [90m-> null[0m[0m
      [31m-[0m[0m private_networks = [] [90m-> null[0m[0m
      [31m-[0m[0m public           = true [90m-> null[0m[0m
      [31m-[0m[0m zone             = "tomaev-maksim.ru." [90m-> null[0m[0m
    }

[1m  # yandex_iam_service_account.bucket-account[0m will be [1m[31mdestroyed[0m
[0m  [31m-[0m[0m resource "yandex_iam_service_account" "bucket-account" {
      [31m-[0m[0m created_at  = "2024-02-05T17:17:32Z" [90m-> null[0m[0m
      [31m-[0m[0m description = "bucket service account" [90m-> null[0m[0m
      [31m-[0m[0m folder_id   = "b1gb1aal3vgk7p7nr6nd" [90m-> null[0m[0m
      [31m-[0m[0m id          = "aje899k49l4ri8fojku7" [90m-> null[0m[0m
      [31m-[0m[0m name        = "bucket-account" [90m-> null[0m[0m
    }

[1m  # yandex_iam_service_account.tech-account[0m will be [1m[31mdestroyed[0m
[0m  [31m-[0m[0m resource "yandex_iam_service_account" "tech-account" {
      [31m-[0m[0m created_at  = "2024-02-05T17:17:33Z" [90m-> null[0m[0m
      [31m-[0m[0m description = "K8S regional service account" [90m-> null[0m[0m
      [31m-[0m[0m folder_id   = "b1gb1aal3vgk7p7nr6nd" [90m-> null[0m[0m
      [31m-[0m[0m id          = "ajep7io5sk5fidb8vvii" [90m-> null[0m[0m
      [31m-[0m[0m name        = "tech-account" [90m-> null[0m[0m
    }

[1m  # yandex_iam_service_account_static_access_key.bucket-account-key[0m will be [1m[31mdestroyed[0m
[0m  [31m-[0m[0m resource "yandex_iam_service_account_static_access_key" "bucket-account-key" {
      [31m-[0m[0m access_key         = "YCAJEe-yunCW29uTxCfp_Emna" [90m-> null[0m[0m
      [31m-[0m[0m created_at         = "2024-02-05T17:17:33Z" [90m-> null[0m[0m
      [31m-[0m[0m description        = "static access key for bucket" [90m-> null[0m[0m
      [31m-[0m[0m id                 = "aje8r7pdjjhqi6up1qum" [90m-> null[0m[0m
      [31m-[0m[0m secret_key         = (sensitive value) [90m-> null[0m[0m
      [31m-[0m[0m service_account_id = "aje899k49l4ri8fojku7" [90m-> null[0m[0m
    }

[1m  # yandex_resourcemanager_folder_iam_member.bucket-account[0m will be [1m[31mdestroyed[0m
[0m  [31m-[0m[0m resource "yandex_resourcemanager_folder_iam_member" "bucket-account" {
      [31m-[0m[0m folder_id = "b1gb1aal3vgk7p7nr6nd" [90m-> null[0m[0m
      [31m-[0m[0m id        = "b1gb1aal3vgk7p7nr6nd/storage.admin/serviceAccount:aje899k49l4ri8fojku7" [90m-> null[0m[0m
      [31m-[0m[0m member    = "serviceAccount:aje899k49l4ri8fojku7" [90m-> null[0m[0m
      [31m-[0m[0m role      = "storage.admin" [90m-> null[0m[0m
    }

[1m  # yandex_resourcemanager_folder_iam_member.k8s-editor[0m will be [1m[31mdestroyed[0m
[0m  [31m-[0m[0m resource "yandex_resourcemanager_folder_iam_member" "k8s-editor" {
      [31m-[0m[0m folder_id = "b1gb1aal3vgk7p7nr6nd" [90m-> null[0m[0m
      [31m-[0m[0m id        = "b1gb1aal3vgk7p7nr6nd/editor/serviceAccount:ajep7io5sk5fidb8vvii" [90m-> null[0m[0m
      [31m-[0m[0m member    = "serviceAccount:ajep7io5sk5fidb8vvii" [90m-> null[0m[0m
      [31m-[0m[0m role      = "editor" [90m-> null[0m[0m
    }

[1m  # yandex_storage_bucket.project-bucket[0m will be [1m[31mdestroyed[0m
[0m  [31m-[0m[0m resource "yandex_storage_bucket" "project-bucket" {
      [31m-[0m[0m access_key            = "YCAJEe-yunCW29uTxCfp_Emna" [90m-> null[0m[0m
      [31m-[0m[0m bucket                = "k8s-tech" [90m-> null[0m[0m
      [31m-[0m[0m bucket_domain_name    = "k8s-tech.storage.yandexcloud.net" [90m-> null[0m[0m
      [31m-[0m[0m default_storage_class = "STANDARD" [90m-> null[0m[0m
      [31m-[0m[0m folder_id             = "b1gb1aal3vgk7p7nr6nd" [90m-> null[0m[0m
      [31m-[0m[0m force_destroy         = false [90m-> null[0m[0m
      [31m-[0m[0m id                    = "k8s-tech" [90m-> null[0m[0m
      [31m-[0m[0m max_size              = 0 [90m-> null[0m[0m
      [31m-[0m[0m secret_key            = (sensitive value) [90m-> null[0m[0m
      [31m-[0m[0m tags                  = {} [90m-> null[0m[0m

      [31m-[0m[0m anonymous_access_flags {
          [31m-[0m[0m config_read = false [90m-> null[0m[0m
          [31m-[0m[0m list        = false [90m-> null[0m[0m
          [31m-[0m[0m read        = false [90m-> null[0m[0m
        }

      [31m-[0m[0m versioning {
          [31m-[0m[0m enabled = false [90m-> null[0m[0m
        }
    }

[1m  # yandex_vpc_network.my-k8s-net[0m will be [1m[31mdestroyed[0m
[0m  [31m-[0m[0m resource "yandex_vpc_network" "my-k8s-net" {
      [31m-[0m[0m created_at                = "2024-02-05T17:17:31Z" [90m-> null[0m[0m
      [31m-[0m[0m default_security_group_id = "enp22qve86t3nff22ps1" [90m-> null[0m[0m
      [31m-[0m[0m folder_id                 = "b1gb1aal3vgk7p7nr6nd" [90m-> null[0m[0m
      [31m-[0m[0m id                        = "enp5ob0gdnqnuq41a4ob" [90m-> null[0m[0m
      [31m-[0m[0m labels                    = {} [90m-> null[0m[0m
      [31m-[0m[0m name                      = "my-k8s-net" [90m-> null[0m[0m
      [31m-[0m[0m subnet_ids                = [
          [31m-[0m[0m "e2l8kesa74sp6dcqgib1",
          [31m-[0m[0m "e9b4dh3mco90q2druajv",
          [31m-[0m[0m "fl84j1g960uevkr0vq2g",
        ] [90m-> null[0m[0m
    }

[1m  # yandex_vpc_subnet.mysubnet-a[0m will be [1m[31mdestroyed[0m
[0m  [31m-[0m[0m resource "yandex_vpc_subnet" "mysubnet-a" {
      [31m-[0m[0m created_at     = "2024-02-05T17:17:33Z" [90m-> null[0m[0m
      [31m-[0m[0m folder_id      = "b1gb1aal3vgk7p7nr6nd" [90m-> null[0m[0m
      [31m-[0m[0m id             = "e9b4dh3mco90q2druajv" [90m-> null[0m[0m
      [31m-[0m[0m labels         = {} [90m-> null[0m[0m
      [31m-[0m[0m name           = "mysubnet-a" [90m-> null[0m[0m
      [31m-[0m[0m network_id     = "enp5ob0gdnqnuq41a4ob" [90m-> null[0m[0m
      [31m-[0m[0m v4_cidr_blocks = [
          [31m-[0m[0m "10.5.0.0/16",
        ] [90m-> null[0m[0m
      [31m-[0m[0m v6_cidr_blocks = [] [90m-> null[0m[0m
      [31m-[0m[0m zone           = "ru-central1-a" [90m-> null[0m[0m
    }

[1m  # yandex_vpc_subnet.mysubnet-b[0m will be [1m[31mdestroyed[0m
[0m  [31m-[0m[0m resource "yandex_vpc_subnet" "mysubnet-b" {
      [31m-[0m[0m created_at     = "2024-02-05T17:17:35Z" [90m-> null[0m[0m
      [31m-[0m[0m folder_id      = "b1gb1aal3vgk7p7nr6nd" [90m-> null[0m[0m
      [31m-[0m[0m id             = "e2l8kesa74sp6dcqgib1" [90m-> null[0m[0m
      [31m-[0m[0m labels         = {} [90m-> null[0m[0m
      [31m-[0m[0m name           = "mysubnet-b" [90m-> null[0m[0m
      [31m-[0m[0m network_id     = "enp5ob0gdnqnuq41a4ob" [90m-> null[0m[0m
      [31m-[0m[0m v4_cidr_blocks = [
          [31m-[0m[0m "10.6.0.0/16",
        ] [90m-> null[0m[0m
      [31m-[0m[0m v6_cidr_blocks = [] [90m-> null[0m[0m
      [31m-[0m[0m zone           = "ru-central1-b" [90m-> null[0m[0m
    }

[1m  # yandex_vpc_subnet.mysubnet-d[0m will be [1m[31mdestroyed[0m
[0m  [31m-[0m[0m resource "yandex_vpc_subnet" "mysubnet-d" {
      [31m-[0m[0m created_at     = "2024-02-05T17:17:34Z" [90m-> null[0m[0m
      [31m-[0m[0m folder_id      = "b1gb1aal3vgk7p7nr6nd" [90m-> null[0m[0m
      [31m-[0m[0m id             = "fl84j1g960uevkr0vq2g" [90m-> null[0m[0m
      [31m-[0m[0m labels         = {} [90m-> null[0m[0m
      [31m-[0m[0m name           = "mysubnet-d" [90m-> null[0m[0m
      [31m-[0m[0m network_id     = "enp5ob0gdnqnuq41a4ob" [90m-> null[0m[0m
      [31m-[0m[0m v4_cidr_blocks = [
          [31m-[0m[0m "10.7.0.0/16",
        ] [90m-> null[0m[0m
      [31m-[0m[0m v6_cidr_blocks = [] [90m-> null[0m[0m
      [31m-[0m[0m zone           = "ru-central1-d" [90m-> null[0m[0m
    }

[1mPlan:[0m 0 to add, 0 to change, 22 to destroy.
[0m
Changes to Outputs:
  [31m-[0m[0m access_key  = "YCAJEe-yunCW29uTxCfp_Emna" [90m-> null[0m[0m
  [31m-[0m[0m external_ip = "84.201.133.127" [90m-> null[0m[0m
  [31m-[0m[0m secret_key  = (sensitive value) [90m-> null[0m[0m
[0m[1mlocal_file.project: Destroying... [id=cf6b65792a288b744ff11709b16e6b69aae4f689][0m[0m
[0m[1mlocal_file.addons: Destroying... [id=385c144e7a968496a47b5b971a467b743788f7a1][0m[0m
[0m[1mlocal_file.k8s-cluster: Destroying... [id=4c4dd90f2e3f246f677966334d8ea237ed1f2d09][0m[0m
[0m[1mlocal_file.project: Destruction complete after 0s[0m
[0m[1mlocal_file.k8s-cluster: Destruction complete after 0s[0m
[0m[1mlocal_file.addons: Destruction complete after 0s[0m
[0m[1myandex_dns_recordset.gitlab: Destroying... [id=dns39tr9k9127j7d38ak/gitlab.tomaev-maksim.ru./A][0m[0m
[0m[1myandex_dns_recordset.kas: Destroying... [id=dns39tr9k9127j7d38ak/kas.tomaev-maksim.ru./A][0m[0m
[0m[1myandex_dns_recordset.grafana: Destroying... [id=dns39tr9k9127j7d38ak/grafana.tomaev-maksim.ru./A][0m[0m
[0m[1myandex_dns_recordset.minio: Destroying... [id=dns39tr9k9127j7d38ak/minio.tomaev-maksim.ru./A][0m[0m
[0m[1myandex_dns_recordset.test-app: Destroying... [id=dns39tr9k9127j7d38ak/test-app.tomaev-maksim.ru./A][0m[0m
[0m[1myandex_dns_recordset.registry: Destroying... [id=dns39tr9k9127j7d38ak/registry.tomaev-maksim.ru./A][0m[0m
[0m[1myandex_storage_bucket.project-bucket: Destroying... [id=k8s-tech][0m[0m
[0m[1myandex_dns_recordset.kas: Destruction complete after 1s[0m
[0m[1myandex_dns_recordset.registry: Destruction complete after 2s[0m
[0m[1myandex_dns_recordset.gitlab: Destruction complete after 2s[0m
[0m[1myandex_dns_recordset.test-app: Destruction complete after 2s[0m
[0m[1myandex_dns_recordset.grafana: Destruction complete after 2s[0m
[0m[1myandex_dns_recordset.minio: Destruction complete after 3s[0m
[0m[1myandex_dns_zone.diplom: Destroying... [id=dns39tr9k9127j7d38ak][0m[0m
[0m[1myandex_compute_instance_group.k8s-node: Destroying... [id=cl1eq71t61q4uqudcs0m][0m[0m
[0m[1myandex_dns_zone.diplom: Destruction complete after 0s[0m
[0m[1myandex_storage_bucket.project-bucket: Still destroying... [id=k8s-tech, 10s elapsed][0m[0m
[0m[1myandex_storage_bucket.project-bucket: Destruction complete after 12s[0m
[0m[1myandex_resourcemanager_folder_iam_member.bucket-account: Destroying... [id=b1gb1aal3vgk7p7nr6nd/storage.admin/serviceAccount:aje899k49l4ri8fojku7][0m[0m
[0m[1myandex_iam_service_account_static_access_key.bucket-account-key: Destroying... [id=aje8r7pdjjhqi6up1qum][0m[0m
[0m[1myandex_iam_service_account_static_access_key.bucket-account-key: Destruction complete after 0s[0m
[0m[1myandex_compute_instance_group.k8s-node: Still destroying... [id=cl1eq71t61q4uqudcs0m, 10s elapsed][0m[0m
[0m[1myandex_resourcemanager_folder_iam_member.bucket-account: Destruction complete after 3s[0m
[0m[1myandex_iam_service_account.bucket-account: Destroying... [id=aje899k49l4ri8fojku7][0m[0m
[0m[1myandex_iam_service_account.bucket-account: Destruction complete after 3s[0m
[0m[1myandex_compute_instance_group.k8s-node: Still destroying... [id=cl1eq71t61q4uqudcs0m, 20s elapsed][0m[0m
[0m[1myandex_compute_instance_group.k8s-node: Still destroying... [id=cl1eq71t61q4uqudcs0m, 30s elapsed][0m[0m
[0m[1myandex_compute_instance_group.k8s-node: Still destroying... [id=cl1eq71t61q4uqudcs0m, 40s elapsed][0m[0m
[0m[1myandex_compute_instance_group.k8s-node: Still destroying... [id=cl1eq71t61q4uqudcs0m, 50s elapsed][0m[0m
[0m[1myandex_compute_instance_group.k8s-node: Still destroying... [id=cl1eq71t61q4uqudcs0m, 1m0s elapsed][0m[0m
[0m[1myandex_compute_instance_group.k8s-node: Destruction complete after 1m9s[0m
[0m[1myandex_resourcemanager_folder_iam_member.k8s-editor: Destroying... [id=b1gb1aal3vgk7p7nr6nd/editor/serviceAccount:ajep7io5sk5fidb8vvii][0m[0m
[0m[1myandex_vpc_subnet.mysubnet-d: Destroying... [id=fl84j1g960uevkr0vq2g][0m[0m
[0m[1myandex_compute_image.ubuntu-test: Destroying... [id=fd8vjq5ab5g28bj5bojc][0m[0m
[0m[1myandex_vpc_subnet.mysubnet-b: Destroying... [id=e2l8kesa74sp6dcqgib1][0m[0m
[0m[1myandex_vpc_subnet.mysubnet-a: Destroying... [id=e9b4dh3mco90q2druajv][0m[0m
[0m[1myandex_vpc_subnet.mysubnet-d: Destruction complete after 2s[0m
[0m[1myandex_resourcemanager_folder_iam_member.k8s-editor: Destruction complete after 3s[0m
[0m[1myandex_iam_service_account.tech-account: Destroying... [id=ajep7io5sk5fidb8vvii][0m[0m
[0m[1myandex_vpc_subnet.mysubnet-b: Destruction complete after 5s[0m
[0m[1myandex_iam_service_account.tech-account: Destruction complete after 3s[0m
[0m[1myandex_vpc_subnet.mysubnet-a: Destruction complete after 8s[0m
[0m[1myandex_vpc_network.my-k8s-net: Destroying... [id=enp5ob0gdnqnuq41a4ob][0m[0m
[0m[1myandex_vpc_network.my-k8s-net: Destruction complete after 0s[0m
[0m[1myandex_compute_image.ubuntu-test: Still destroying... [id=fd8vjq5ab5g28bj5bojc, 10s elapsed][0m[0m
[0m[1myandex_compute_image.ubuntu-test: Still destroying... [id=fd8vjq5ab5g28bj5bojc, 20s elapsed][0m[0m
[0m[1myandex_compute_image.ubuntu-test: Destruction complete after 24s[0m
[0m[1m[32m
Destroy complete! Resources: 22 destroyed.
[0m